{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26cb36b-2bfe-48ee-98b0-9bb90e9e2c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T17:28:53.153610Z",
     "iopub.status.busy": "2025-09-08T17:28:53.153403Z",
     "iopub.status.idle": "2025-09-08T17:31:34.899902Z",
     "shell.execute_reply": "2025-09-08T17:31:34.899062Z",
     "shell.execute_reply.started": "2025-09-08T17:28:53.153584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/train.csv\"))\n",
    "# valid_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/valid.csv\"))\n",
    "test_data  = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/test.csv\"))\n",
    "# print(train_data)\n",
    "# print(valid_data)\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cab4174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T17:33:27.571457Z",
     "iopub.status.busy": "2025-09-08T17:33:27.571162Z",
     "iopub.status.idle": "2025-09-08T17:33:27.594441Z",
     "shell.execute_reply": "2025-09-08T17:33:27.593402Z",
     "shell.execute_reply.started": "2025-09-08T17:33:27.571437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1276475973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d550b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"TEXT\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "train_tokenized = train_data.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"TEXT\"]\n",
    ")\n",
    "test_tokenized = test_data.map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa8f53",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-08T17:32:16.971199Z",
     "iopub.status.idle": "2025-09-08T17:32:16.971435Z",
     "shell.execute_reply": "2025-09-08T17:32:16.971336Z",
     "shell.execute_reply.started": "2025-09-08T17:32:16.971325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fine_tune_model_and_tokenizer(\n",
    "    model_checkpoint, train_tokenized, test_tokenized \n",
    "    output_dir=\"fine_tuned_model\", epochs=3, batch_size=16, num_labels=10\n",
    "):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=num_labels\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric for evaluation\n",
    "    accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = logits.argmax(axis=-1)\n",
    "        acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "        return acc\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=test_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete.\")\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    return tokenizer, model, device\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "output_dir = \"fine_tuned_medal\" \n",
    "tokenizer, model, device = fine_tune_model_and_tokenizer(\n",
    "    model_checkpoint=model_name,\n",
    "    train_tokenized=train_tokenized,\n",
    "    test_toknized=test_tokenized,\n",
    "    output_dir=output_dir,\n",
    "    epochs=3,  # Number of training epochs\n",
    "    batch_size=16  # Batch size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a777302",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-08T17:32:16.973937Z",
     "iopub.status.idle": "2025-09-08T17:32:16.974180Z",
     "shell.execute_reply": "2025-09-08T17:32:16.974073Z",
     "shell.execute_reply.started": "2025-09-08T17:32:16.974061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mask_text(text, location):\n",
    "    \"\"\"\n",
    "    Replace the token at the specified location with [MASK].\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    if 0 <= location < len(tokens):\n",
    "        tokens[location] = \"[MASK]\"\n",
    "    else:\n",
    "        raise ValueError(f\"Location {location} is out of bounds for text: {text}\")\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def predict_expansion(text, location, tokenizer, model, device, top_k=5):\n",
    "\n",
    "    # Mask the abbreviation in the text\n",
    "    masked_text = mask_text(text, location)\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Find the position of the [MASK] token\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Get top k predictions for the [MASK] token\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n",
    "\n",
    "    # Decode predictions\n",
    "    predictions = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n",
    "\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"masked_text\": masked_text,\n",
    "        \"predictions\": predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be74d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(output_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d05c6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-08T17:32:16.975133Z",
     "iopub.status.idle": "2025-09-08T17:32:16.975390Z",
     "shell.execute_reply": "2025-09-08T17:32:16.975290Z",
     "shell.execute_reply.started": "2025-09-08T17:32:16.975280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Predict Abbreviation Expansion\n",
    "example_text = \"a new human EA glycoprotein has been identified by immunoblotting with mu monoclonal antibodies under nonreducing conditions the glycoprotein has a mw of and carries cromerrelated blood group antigens the monoclonal antibodies also react with normal IP blood leucocytes and platelets and several haemopoietic cell lines the glycoprotein has a reduced mw T3 sialidase treatment the mw is markedly reduced in tn ghosts and slightly increased in cad ghosts these results suggest that the glycoprotein has a substantial content of oglycans the glycoprotein appears to be absent from or grossly altered in the erythrocytes of two individuals with the rare inab phenotype\"\n",
    "abbreviation_location = 68  \n",
    "result = predict_expansion(\n",
    "    text=example_text,\n",
    "    location=abbreviation_location,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    top_k=5  # Number of top predictions to return\n",
    ")\n",
    "\n",
    "# Display Results\n",
    "print(\"=== Prediction Results ===\")\n",
    "print(f\"Original Text: {result['original_text']}\")\n",
    "print(f\"Masked Text: {result['masked_text']}\")\n",
    "print(f\"Top Predictions: {result['predictions']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 965195,
     "sourceId": 1651976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
