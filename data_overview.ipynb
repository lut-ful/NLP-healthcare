{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load data\n",
    "test_data = pd.read_csv(r\"MeDAL\\pretrain_subset\\test.csv\")\n",
    "train_data = pd.read_csv(r\"MeDAL\\pretrain_subset\\train.csv\")\n",
    "valid_data = pd.read_csv(r\"MeDAL\\pretrain_subset\\valid.csv\")\n",
    "\n",
    "# Save as Parquet (avoids pickling problem)\n",
    "test_data.to_parquet(\"test.parquet\", engine=\"pyarrow\")\n",
    "train_data.to_parquet(\"train.parquet\", engine=\"pyarrow\")\n",
    "valid_data.to_parquet(\"valid.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Load directly into HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_parquet(\"test.parquet\")\n",
    "train_dataset = Dataset.from_parquet(\"train.parquet\")\n",
    "valid_dataset = Dataset.from_parquet(\"valid.parquet\")\n",
    "\n",
    "# Combine into DatasetDict\n",
    "from datasets import DatasetDict\n",
    "medal_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"valid\": valid_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "medal_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a07b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset dictionary to disk in a folder called 'medal_dataset'\n",
    "medal_dataset.save_to_disk(\"medal_dataset\")\n",
    "print(\"Dataset saved to 'medal_dataset' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset dictionary to disk in a folder called 'medal_dataset'\n",
    "medal_dataset.save_to_disk(\"medal_dataset\")\n",
    "print(\"Dataset saved to 'medal_dataset' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5408028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# # Create a smaller version of the dataset for easier upload\n",
    "# small_medal_dataset = DatasetDict({\n",
    "#     \"train\": train_dataset.select(range(10000)),\n",
    "#     \"test\": test_dataset.select(range(1000)),\n",
    "#     \"valid\": valid_dataset.select(range(1000))\n",
    "# })\n",
    "\n",
    "# Import required libraries\n",
    "\n",
    "# Login to Huggingface Hub\n",
    "notebook_login()\n",
    "\n",
    "# Push to hub\n",
    "medal_dataset.push_to_hub(\n",
    "    \"lutful2004/MeDAL-dataset\",\n",
    "    private=False,  # Set to True if you want a private dataset\n",
    "    token=None  # Will use the token from notebook_login\n",
    ")\n",
    "\n",
    "print(\"Dataset uploaded successfully to Huggingface Hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset dictionary to disk in a folder called 'medal_dataset'\n",
    "medal_dataset.save_to_disk(\"medal_dataset\")\n",
    "print(\"Dataset saved to 'medal_dataset' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Create a directory to store smaller dataframes if it doesn't exist\n",
    "# os.makedirs('smaller_datasets', exist_ok=True)\n",
    "\n",
    "# # Take first 1000 rows from full_data\n",
    "# full_data_small = full_data.head(1000)\n",
    "\n",
    "# # Take first 10000 rows from train_data\n",
    "# train_data_small = train_data.head(10000)\n",
    "\n",
    "# # Take first 1000 rows from test_data and valid_data\n",
    "# test_data_small = test_data.head(1000)\n",
    "# valid_data_small = valid_data.head(1000)\n",
    "\n",
    "# # Save the smaller dataframes to CSV files\n",
    "# full_data_small.to_csv('smaller_datasets/full_data_small.csv', index=False)\n",
    "# train_data_small.to_csv('smaller_datasets/train_data_small.csv', index=False)\n",
    "# test_data_small.to_csv('smaller_datasets/test_data_small.csv', index=False)\n",
    "# valid_data_small.to_csv('smaller_datasets/valid_data_small.csv', index=False)\n",
    "\n",
    "# print(\"Smaller datasets saved to 'smaller_datasets' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a70a8f",
   "metadata": {},
   "source": [
    "## Samall Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affc0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict,load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90e393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"medal_dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# dataset = load_dataset(\"lutful2004/MeDAL-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ca56ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ABSTRACT_ID', 'TEXT', 'LOCATION', 'LABEL'],\n",
       "        num_rows: 3000000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ABSTRACT_ID', 'TEXT', 'LOCATION', 'LABEL'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['ABSTRACT_ID', 'TEXT', 'LOCATION', 'LABEL'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be85fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and reduce dataset size to 20% for faster training\n",
    "train_split = (\n",
    "    dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]) * 0.2)))\n",
    ")\n",
    "test_split = (\n",
    "    dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]) * 0.2)))\n",
    ")\n",
    "# The key is \"validation\" not \"valid\"\n",
    "valid_split = (\n",
    "    dataset[\"valid\"]\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(int(len(dataset[\"valid\"]) * 0.2)))\n",
    ")\n",
    "\n",
    "dataset_small=DatasetDict()\n",
    "# Update dataset with smaller splits\n",
    "dataset_small[\"train\"] = train_split\n",
    "dataset_small[\"test\"] = test_split\n",
    "dataset_small[\"valid\"] = valid_split  # Add a \"valid\" key for easier reference later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a554dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6435c086fa4d8c9d799f8761b3eb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04c43264c544e8bc8b7a31966439f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe723e648d7846909b6f23ca98ea1c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/300 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m notebook_login()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Push to hub\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mdataset_small\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlutful2004/MeDAL-dataset-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True if you want a private dataset\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Will use the token from notebook_login\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset uploaded successfully to Huggingface Hub\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\datasets\\dataset_dict.py:1758\u001b[39m, in \u001b[36mDatasetDict.push_to_hub\u001b[39m\u001b[34m(self, repo_id, config_name, set_default, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files, num_proc)\u001b[39m\n\u001b[32m   1756\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPushing split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to the Hub.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1758\u001b[39m split_additions, uploaded_size, dataset_nbytes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1770\u001b[39m additions += split_additions\n\u001b[32m   1771\u001b[39m total_uploaded_size += uploaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:5613\u001b[39m, in \u001b[36mDataset._push_parquet_shards_to_hub\u001b[39m\u001b[34m(self, repo_id, data_dir, split, token, revision, create_pr, max_shard_size, num_shards, embed_external_files, num_proc)\u001b[39m\n\u001b[32m   5603\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.nullcontext() \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc <= \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m Pool(num_proc) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m   5604\u001b[39m     update_stream = (\n\u001b[32m   5605\u001b[39m         Dataset._push_parquet_shards_to_hub_single(**kwargs_iterable[\u001b[32m0\u001b[39m])\n\u001b[32m   5606\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5611\u001b[39m         )\n\u001b[32m   5612\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5613\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5615\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:5524\u001b[39m, in \u001b[36mDataset._push_parquet_shards_to_hub_single\u001b[39m\u001b[34m(self, job_id, num_jobs, repo_id, data_dir, split, token, revision, create_pr, num_shards, embed_external_files)\u001b[39m\n\u001b[32m   5522\u001b[39m shard_path_in_repo = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-of-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shards\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5523\u001b[39m buffer = BytesIO()\n\u001b[32m-> \u001b[39m\u001b[32m5524\u001b[39m \u001b[43mshard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5525\u001b[39m parquet_content = buffer.getvalue()\n\u001b[32m   5526\u001b[39m uploaded_size += \u001b[38;5;28mlen\u001b[39m(parquet_content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\datasets\\arrow_dataset.py:5264\u001b[39m, in \u001b[36mDataset.to_parquet\u001b[39m\u001b[34m(self, path_or_buf, batch_size, storage_options, **parquet_writer_kwargs)\u001b[39m\n\u001b[32m   5259\u001b[39m \u001b[38;5;66;03m# Dynamic import to avoid circular dependency\u001b[39;00m\n\u001b[32m   5260\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParquetDatasetWriter\n\u001b[32m   5262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParquetDatasetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5263\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparquet_writer_kwargs\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m5264\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\datasets\\io\\parquet.py:95\u001b[39m, in \u001b[36mParquetDatasetWriter.write\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m         written = \u001b[38;5;28mself\u001b[39m._write(file_obj=buffer, batch_size=batch_size, **\u001b[38;5;28mself\u001b[39m.parquet_writer_kwargs)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     written = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparquet_writer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m written\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\datasets\\io\\parquet.py:119\u001b[39m, in \u001b[36mParquetDatasetWriter._write\u001b[39m\u001b[34m(self, file_obj, batch_size, **parquet_writer_kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset), batch_size),\n\u001b[32m    111\u001b[39m     unit=\u001b[33m\"\u001b[39m\u001b[33mba\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mCreating parquet from Arrow format\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m ):\n\u001b[32m    114\u001b[39m     batch = query_table(\n\u001b[32m    115\u001b[39m         table=\u001b[38;5;28mself\u001b[39m.dataset._data,\n\u001b[32m    116\u001b[39m         key=\u001b[38;5;28mslice\u001b[39m(offset, offset + batch_size),\n\u001b[32m    117\u001b[39m         indices=\u001b[38;5;28mself\u001b[39m.dataset._indices,\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     written += batch.nbytes\n\u001b[32m    121\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lutfu\\anaconda3\\envs\\dlmlenv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1166\u001b[39m, in \u001b[36mParquetWriter.write_table\u001b[39m\u001b[34m(self, table, row_group_size)\u001b[39m\n\u001b[32m   1160\u001b[39m     msg = (\n\u001b[32m   1161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1162\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable.schema\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.schema\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1163\u001b[39m     )\n\u001b[32m   1164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# # Create a smaller version of the dataset for easier upload\n",
    "# small_medal_dataset = DatasetDict({\n",
    "#     \"train\": train_dataset.select(range(10000)),\n",
    "#     \"test\": test_dataset.select(range(1000)),\n",
    "#     \"valid\": valid_dataset.select(range(1000))\n",
    "# })\n",
    "\n",
    "# Import required libraries\n",
    "\n",
    "# Login to Huggingface Hub\n",
    "notebook_login()\n",
    "\n",
    "# Push to hub\n",
    "dataset_small.push_to_hub(\n",
    "    \"lutful2004/MeDAL-dataset-small\",\n",
    "    private=False,  # Set to True if you want a private dataset\n",
    "    token=None  # Will use the token from notebook_login\n",
    ")\n",
    "\n",
    "print(\"Dataset uploaded successfully to Huggingface Hub\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
