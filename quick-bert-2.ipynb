{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1651976,"sourceType":"datasetVersion","datasetId":965195}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d26cb36b-2bfe-48ee-98b0-9bb90e9e2c01","cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\ntrain_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/train.csv\"))\n# valid_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/valid.csv\"))\ntest_data  = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/test.csv\"))\n# print(train_data)\n# print(valid_data)\n# print(test_data)","metadata":{"execution":{"iopub.status.busy":"2025-09-08T20:09:03.981149Z","iopub.execute_input":"2025-09-08T20:09:03.981433Z","iopub.status.idle":"2025-09-08T20:11:25.867774Z","shell.execute_reply.started":"2025-09-08T20:09:03.981408Z","shell.execute_reply":"2025-09-08T20:11:25.867148Z"},"trusted":true},"outputs":[],"execution_count":1},{"id":"2cab4174","cell_type":"code","source":"from transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding\n)\nimport torch\nfrom datasets import Dataset\nimport evaluate\n","metadata":{"execution":{"iopub.status.busy":"2025-09-08T20:11:25.871798Z","iopub.execute_input":"2025-09-08T20:11:25.872370Z","iopub.status.idle":"2025-09-08T20:11:52.358900Z","shell.execute_reply.started":"2025-09-08T20:11:25.872350Z","shell.execute_reply":"2025-09-08T20:11:52.358317Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-09-08 20:11:43.235089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757362303.616350     165 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757362303.708776     165 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"0d550b74","cell_type":"code","source":"# Tokenization\nmodel_name = \"bert-base-uncased\"\noutput_dir = \"fine_tuned_medal\" \ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"TEXT\"], truncation=True, padding=\"max_length\", max_length=128\n    )\n\n\ntrain_tokenized = train_data.select(range(100)).map(\n    tokenize_function, batched=True, remove_columns=[\"TEXT\"]\n)\ntest_tokenized = test_data.select(range(50)).map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:14:48.714942Z","iopub.execute_input":"2025-09-08T20:14:48.715290Z","iopub.status.idle":"2025-09-08T20:14:49.107517Z","shell.execute_reply.started":"2025-09-08T20:14:48.715263Z","shell.execute_reply":"2025-09-08T20:14:49.106682Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4566231ddbd4a9684f0c45d0e415425"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c22612762f4e2a9c5f480cfc7f46e2"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('fine_tuned_medal/tokenizer_config.json',\n 'fine_tuned_medal/special_tokens_map.json',\n 'fine_tuned_medal/vocab.txt',\n 'fine_tuned_medal/added_tokens.json',\n 'fine_tuned_medal/tokenizer.json')"},"metadata":{}}],"execution_count":9},{"id":"c99d0816-ff0a-40b3-981a-a21255c9ec83","cell_type":"code","source":"def fine_tune_model_and_tokenizer(\n    model_checkpoint, train_tokenized, test_tokenized,\n    output_dir=\"fine_tuned_model\", epochs=3, batch_size=16, num_labels=10\n):\n    # Load model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_checkpoint, num_labels=num_labels\n    )\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Data collator\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # Metric for evaluation\n    accuracy_metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = logits.argmax(axis=-1)\n        return accuracy_metric.compute(predictions=predictions, references=labels)\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        save_steps=500,\n        learning_rate=5e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=epochs,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        fp16=torch.cuda.is_available()\n    )\n\n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_tokenized,\n        eval_dataset=test_tokenized,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    print(\"Fine-tuning complete.\")\n\n    # Save model + tokenizer\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    return model, device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:21:07.452397Z","iopub.execute_input":"2025-09-08T20:21:07.452712Z","iopub.status.idle":"2025-09-08T20:21:07.460078Z","shell.execute_reply.started":"2025-09-08T20:21:07.452688Z","shell.execute_reply":"2025-09-08T20:21:07.459201Z"}},"outputs":[],"execution_count":21},{"id":"7dfa8f53","cell_type":"code","source":"\nmodel, device = fine_tune_model_and_tokenizer(\n    model_checkpoint=model_name,\n    train_tokenized=train_tokenized,\n    test_tokenized=test_tokenized,\n    output_dir=output_dir,\n    epochs=3,  # Number of training epochs\n    batch_size=16  # Batch size\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:21:09.178451Z","iopub.execute_input":"2025-09-08T20:21:09.179294Z","iopub.status.idle":"2025-09-08T20:21:09.941047Z","shell.execute_reply.started":"2025-09-08T20:21:09.179259Z","shell.execute_reply":"2025-09-08T20:21:09.939993Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_165/2761051294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, device = fine_tune_model_and_tokenizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_tokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tokenized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_tokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_tokenized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_165/4200775584.py\u001b[0m in \u001b[0;36mfine_tune_model_and_tokenizer\u001b[0;34m(model_checkpoint, train_tokenized, test_tokenized, output_dir, epochs, batch_size, num_labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"],"ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","output_type":"error"}],"execution_count":22},{"id":"1a777302","cell_type":"code","source":"def mask_text(text, location):\n    \"\"\"\n    Replace the token at the specified location with [MASK].\n    \"\"\"\n    tokens = text.split()\n    if 0 <= location < len(tokens):\n        tokens[location] = \"[MASK]\"\n    else:\n        raise ValueError(f\"Location {location} is out of bounds for text: {text}\")\n    return \" \".join(tokens)\n\ndef predict_expansion(text, location, tokenizer, model, device, top_k=5):\n\n    # Mask the abbreviation in the text\n    masked_text = mask_text(text, location)\n    \n    # Tokenize the input\n    inputs = tokenizer(masked_text, return_tensors=\"pt\", padding=True, truncation=True)\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n    \n    # Predict the masked token\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    # Find the position of the [MASK] token\n    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n\n    # Get top k predictions for the [MASK] token\n    mask_token_logits = logits[0, mask_token_index, :]\n    top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n\n    # Decode predictions\n    predictions = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n\n    return {\n        \"original_text\": text,\n        \"masked_text\": masked_text,\n        \"predictions\": predictions,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:15:03.822195Z","iopub.status.idle":"2025-09-08T20:15:03.822596Z","shell.execute_reply.started":"2025-09-08T20:15:03.822408Z","shell.execute_reply":"2025-09-08T20:15:03.822426Z"}},"outputs":[],"execution_count":null},{"id":"9be74d5e","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel = AutoModelForMaskedLM.from_pretrained(output_dir)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:15:03.823963Z","iopub.status.idle":"2025-09-08T20:15:03.824259Z","shell.execute_reply.started":"2025-09-08T20:15:03.824101Z","shell.execute_reply":"2025-09-08T20:15:03.824113Z"}},"outputs":[],"execution_count":null},{"id":"8b2d05c6","cell_type":"code","source":"\n# Step 3: Predict Abbreviation Expansion\nexample_text = \"a new human EA glycoprotein has been identified by immunoblotting with mu monoclonal antibodies under nonreducing conditions the glycoprotein has a mw of and carries cromerrelated blood group antigens the monoclonal antibodies also react with normal IP blood leucocytes and platelets and several haemopoietic cell lines the glycoprotein has a reduced mw T3 sialidase treatment the mw is markedly reduced in tn ghosts and slightly increased in cad ghosts these results suggest that the glycoprotein has a substantial content of oglycans the glycoprotein appears to be absent from or grossly altered in the erythrocytes of two individuals with the rare inab phenotype\"\nabbreviation_location = 68  \nresult = predict_expansion(\n    text=example_text,\n    location=abbreviation_location,\n    tokenizer=tokenizer,\n    model=model,\n    device=device,\n    top_k=5  # Number of top predictions to return\n)\n\n# Display Results\nprint(\"=== Prediction Results ===\")\nprint(f\"Original Text: {result['original_text']}\")\nprint(f\"Masked Text: {result['masked_text']}\")\nprint(f\"Top Predictions: {result['predictions']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:15:03.824855Z","iopub.status.idle":"2025-09-08T20:15:03.825152Z","shell.execute_reply.started":"2025-09-08T20:15:03.824991Z","shell.execute_reply":"2025-09-08T20:15:03.825005Z"}},"outputs":[],"execution_count":null}]}