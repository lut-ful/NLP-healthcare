{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1651976,"sourceType":"datasetVersion","datasetId":965195}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d26cb36b-2bfe-48ee-98b0-9bb90e9e2c01","cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\ntrain_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/train.csv\"))\n# valid_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/valid.csv\"))\ntest_data  = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/test.csv\"))\n# print(train_data)\n# print(valid_data)\n# print(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2cab4174","cell_type":"code","source":"from transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding\n)\nimport torch\nfrom datasets import Dataset\nimport evaluate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0d550b74","cell_type":"code","source":"# Tokenization\nmodel_name = \"bert-base-uncased\"\noutput_dir = \"fine_tuned_medal\" \ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"TEXT\"], truncation=True, padding=\"max_length\", max_length=128\n    )\n\n\ntrain_tokenized = train_data.map(\n    tokenize_function, batched=True, remove_columns=[\"TEXT\"]\n)\ntest_tokenized = test_data.map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7dfa8f53","cell_type":"code","source":"\ndef fine_tune_model_and_tokenizer(\n    model_checkpoint, train_tokenized, test_tokenized \n    output_dir=\"fine_tuned_model\", epochs=3, batch_size=16, num_labels=10\n):\n    # Load tokenizer and model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_checkpoint, num_labels=num_labels\n    )\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Data collator\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # Metric for evaluation\n    accuracy_metric = load_metric(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = logits.argmax(axis=-1)\n        acc = accuracy_metric.compute(predictions=predictions, references=labels)\n        return acc\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        save_steps=500,\n        learning_rate=5e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=epochs,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        fp16=torch.cuda.is_available()\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_tokenized,\n        eval_dataset=test_tokenized,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    print(\"Fine-tuning complete.\")\n\n    # Save model and tokenizer\n    trainer.save_model(output_dir)\n\n\n    return tokenizer, model, device\n\n\ntokenizer, model, device = fine_tune_model_and_tokenizer(\n    model_checkpoint=model_name,\n    train_tokenized=train_tokenized,\n    test_toknized=test_tokenized,\n    output_dir=output_dir,\n    epochs=3,  # Number of training epochs\n    batch_size=16  # Batch size\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a777302","cell_type":"code","source":"def mask_text(text, location):\n    \"\"\"\n    Replace the token at the specified location with [MASK].\n    \"\"\"\n    tokens = text.split()\n    if 0 <= location < len(tokens):\n        tokens[location] = \"[MASK]\"\n    else:\n        raise ValueError(f\"Location {location} is out of bounds for text: {text}\")\n    return \" \".join(tokens)\n\ndef predict_expansion(text, location, tokenizer, model, device, top_k=5):\n\n    # Mask the abbreviation in the text\n    masked_text = mask_text(text, location)\n    \n    # Tokenize the input\n    inputs = tokenizer(masked_text, return_tensors=\"pt\", padding=True, truncation=True)\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n    \n    # Predict the masked token\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    # Find the position of the [MASK] token\n    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n\n    # Get top k predictions for the [MASK] token\n    mask_token_logits = logits[0, mask_token_index, :]\n    top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n\n    # Decode predictions\n    predictions = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n\n    return {\n        \"original_text\": text,\n        \"masked_text\": masked_text,\n        \"predictions\": predictions,\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9be74d5e","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel = AutoModelForMaskedLM.from_pretrained(output_dir)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8b2d05c6","cell_type":"code","source":"\n# Step 3: Predict Abbreviation Expansion\nexample_text = \"a new human EA glycoprotein has been identified by immunoblotting with mu monoclonal antibodies under nonreducing conditions the glycoprotein has a mw of and carries cromerrelated blood group antigens the monoclonal antibodies also react with normal IP blood leucocytes and platelets and several haemopoietic cell lines the glycoprotein has a reduced mw T3 sialidase treatment the mw is markedly reduced in tn ghosts and slightly increased in cad ghosts these results suggest that the glycoprotein has a substantial content of oglycans the glycoprotein appears to be absent from or grossly altered in the erythrocytes of two individuals with the rare inab phenotype\"\nabbreviation_location = 68  \nresult = predict_expansion(\n    text=example_text,\n    location=abbreviation_location,\n    tokenizer=tokenizer,\n    model=model,\n    device=device,\n    top_k=5  # Number of top predictions to return\n)\n\n# Display Results\nprint(\"=== Prediction Results ===\")\nprint(f\"Original Text: {result['original_text']}\")\nprint(f\"Masked Text: {result['masked_text']}\")\nprint(f\"Top Predictions: {result['predictions']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}