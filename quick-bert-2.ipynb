{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model_and_tokenizer(\n",
    "    model_checkpoint, train_data:Dataset,valid_data:Dataset,test_data:Dataset, output_dir=\"fine_tuned_model\", epochs=10, batch_size=32\n",
    "):\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model=AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "    device=torch.device(\"cude\" if torch.cude.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # train_data = Dataset.from_pandas(pd.read_csv(medal_dataset_path))\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[\"TEXT\"],truncation=True,padding=\"max_length\",max_length=128\n",
    "        )\n",
    "    train_tokenized = train_data.map(tokenize_function, batched=True, remove_columns=[\"TEXT\", \"LABEL\"])\n",
    "    valid_tokenized = valid_data.map(tokenize_function, batched=True, remove_columns=[\"TEXT\", \"LABEL\"])\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,mlm=True,mlm_probability=0.15\n",
    "    )\n",
    "    \n",
    "    training_args=TrainingArguments(\n",
    "        output_dir=output_dir, \n",
    "        overwrite_output_dir=True, \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size, \n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=500,\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting fine-tunng... \")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete.\")\n",
    "    \n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return tokenizer, model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ecb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_text(text, location):\n",
    "    \"\"\"\n",
    "    Replace the token at the specified location with [MASK].\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    if 0 <= location < len(tokens):\n",
    "        tokens[location] = \"[MASK]\"\n",
    "    else:\n",
    "        raise ValueError(f\"Location {location} is out of bounds for text: {text}\")\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a777302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_expansion(text, location, tokenizer, model, device, top_k=5):\n",
    "\n",
    "    # Mask the abbreviation in the text\n",
    "    masked_text = mask_text(text, location)\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Find the position of the [MASK] token\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Get top k predictions for the [MASK] token\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n",
    "\n",
    "    # Decode predictions\n",
    "    predictions = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n",
    "\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"masked_text\": masked_text,\n",
    "        \"predictions\": predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71664e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Replace <path_or_url> with the actual test.csv URL from Zenodo\n",
    "train_data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"https://zenodo.org/record/4276178/files/train.csv\"\n",
    ")\n",
    "test_data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"https://zenodo.org/record/4276178/files/test.csv\"\n",
    ")\n",
    "valid_data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"https://zenodo.org/record/4276178/files/valid.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, device = fine_tune_model_and_tokenizer(\n",
    "    model_checkpoint=model_checkpoint,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    output_dir=output_dir,\n",
    "    epochs=3,  # Number of training epochs\n",
    "    batch_size=16  # Batch size\n",
    ")\n",
    "\n",
    "# Step 2: Load the Fine-Tuned Model\n",
    "# Reload the fine-tuned model and tokenizer for prediction\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(output_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Step 3: Predict Abbreviation Expansion\n",
    "\n",
    "example_text = \"a new human EA glycoprotein has been identified by immunoblotting with mu monoclonal antibodies under nonreducing conditions the glycoprotein has a mw of and carries cromerrelated blood group antigens the monoclonal antibodies also react with normal IP blood leucocytes and platelets and several haemopoietic cell lines the glycoprotein has a reduced mw T3 sialidase treatment the mw is markedly reduced in tn ghosts and slightly increased in cad ghosts these results suggest that the glycoprotein has a substantial content of oglycans the glycoprotein appears to be absent from or grossly altered in the erythrocytes of two individuals with the rare inab phenotype\"\n",
    "abbreviation_location = 68  \n",
    "\n",
    "# Use the predict_expansion function\n",
    "result = predict_expansion(\n",
    "    text=example_text,\n",
    "    location=abbreviation_location,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    top_k=5  # Number of top predictions to return\n",
    ")\n",
    "\n",
    "# Display Results\n",
    "print(\"=== Prediction Results ===\")\n",
    "print(f\"Original Text: {result['original_text']}\")\n",
    "print(f\"Masked Text: {result['masked_text']}\")\n",
    "print(f\"Top Predictions: {result['predictions']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
