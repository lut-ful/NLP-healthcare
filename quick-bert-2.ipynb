{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1651976,"sourceType":"datasetVersion","datasetId":965195}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d26cb36b-2bfe-48ee-98b0-9bb90e9e2c01","cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\ntrain_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/train.csv\"))\n# valid_data = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/valid.csv\"))\ntest_data  = Dataset.from_pandas(pd.read_csv(\"/kaggle/input/medal-emnlp/pretrain_subset/test.csv\"))\n# print(train_data)\n# print(valid_data)\n# print(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:28:53.153403Z","iopub.execute_input":"2025-09-08T17:28:53.153610Z","iopub.status.idle":"2025-09-08T17:31:34.899902Z","shell.execute_reply.started":"2025-09-08T17:28:53.153584Z","shell.execute_reply":"2025-09-08T17:31:34.899062Z"}},"outputs":[],"execution_count":1},{"id":"2cab4174","cell_type":"code","source":"from transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding\n)\nimport torch\nfrom datasets import Dataset\nimport evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:33:27.571162Z","iopub.execute_input":"2025-09-08T17:33:27.571457Z","iopub.status.idle":"2025-09-08T17:33:27.594441Z","shell.execute_reply.started":"2025-09-08T17:33:27.571437Z","shell.execute_reply":"2025-09-08T17:33:27.593402Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1276475973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"],"ename":"ModuleNotFoundError","evalue":"No module named 'evaluate'","output_type":"error"}],"execution_count":3},{"id":"7dfa8f53","cell_type":"code","source":"\ndef fine_tune_model_and_tokenizer(\n    model_checkpoint, train_data: Dataset, test_data: Dataset, \n    output_dir=\"fine_tuned_model\", epochs=3, batch_size=16, num_labels=10\n):\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_checkpoint, num_labels=num_labels\n    )\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Tokenization\n    def tokenize_function(example):\n        return tokenizer(\n            example[\"TEXT\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=128\n        )\n\n    train_tokenized = train_data.map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])\n    test_tokenized  = test_data.map(tokenize_function, batched=True, remove_columns=[\"TEXT\"])\n\n    # Data collator\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # Metric for evaluation\n    accuracy_metric = load_metric(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = logits.argmax(axis=-1)\n        acc = accuracy_metric.compute(predictions=predictions, references=labels)\n        return acc\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        save_steps=500,\n        learning_rate=5e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=epochs,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        fp16=torch.cuda.is_available()\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_tokenized,\n        eval_dataset=test_tokenized,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    print(\"Fine-tuning complete.\")\n\n    # Save model and tokenizer\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    return tokenizer, model, device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:32:16.971199Z","iopub.status.idle":"2025-09-08T17:32:16.971435Z","shell.execute_reply.started":"2025-09-08T17:32:16.971325Z","shell.execute_reply":"2025-09-08T17:32:16.971336Z"}},"outputs":[],"execution_count":null},{"id":"190ecb72","cell_type":"code","source":"def mask_text(text, location):\n    \"\"\"\n    Replace the token at the specified location with [MASK].\n    \"\"\"\n    tokens = text.split()\n    if 0 <= location < len(tokens):\n        tokens[location] = \"[MASK]\"\n    else:\n        raise ValueError(f\"Location {location} is out of bounds for text: {text}\")\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:32:16.972696Z","iopub.status.idle":"2025-09-08T17:32:16.973011Z","shell.execute_reply.started":"2025-09-08T17:32:16.972889Z","shell.execute_reply":"2025-09-08T17:32:16.972902Z"}},"outputs":[],"execution_count":null},{"id":"1a777302","cell_type":"code","source":"def predict_expansion(text, location, tokenizer, model, device, top_k=5):\n\n    # Mask the abbreviation in the text\n    masked_text = mask_text(text, location)\n    \n    # Tokenize the input\n    inputs = tokenizer(masked_text, return_tensors=\"pt\", padding=True, truncation=True)\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n    \n    # Predict the masked token\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    # Find the position of the [MASK] token\n    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n\n    # Get top k predictions for the [MASK] token\n    mask_token_logits = logits[0, mask_token_index, :]\n    top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n\n    # Decode predictions\n    predictions = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n\n    return {\n        \"original_text\": text,\n        \"masked_text\": masked_text,\n        \"predictions\": predictions,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:32:16.973937Z","iopub.status.idle":"2025-09-08T17:32:16.974180Z","shell.execute_reply.started":"2025-09-08T17:32:16.974061Z","shell.execute_reply":"2025-09-08T17:32:16.974073Z"}},"outputs":[],"execution_count":null},{"id":"8b2d05c6","cell_type":"code","source":"model_name = \"bert-base-uncased\"\noutput_dir = \"fine_tuned_medal\" \ntokenizer, model, device = fine_tune_model_and_tokenizer(\n    model_checkpoint=model_name,\n    train_data=train_data,\n    test_data=test_data,\n    # valid_data=valid_data,\n    output_dir=output_dir,\n    epochs=3,  # Number of training epochs\n    batch_size=16  # Batch size\n)\n\n# Step 2: Load the Fine-Tuned Model\n# Reload the fine-tuned model and tokenizer for prediction\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel = AutoModelForMaskedLM.from_pretrained(output_dir)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Step 3: Predict Abbreviation Expansion\n\nexample_text = \"a new human EA glycoprotein has been identified by immunoblotting with mu monoclonal antibodies under nonreducing conditions the glycoprotein has a mw of and carries cromerrelated blood group antigens the monoclonal antibodies also react with normal IP blood leucocytes and platelets and several haemopoietic cell lines the glycoprotein has a reduced mw T3 sialidase treatment the mw is markedly reduced in tn ghosts and slightly increased in cad ghosts these results suggest that the glycoprotein has a substantial content of oglycans the glycoprotein appears to be absent from or grossly altered in the erythrocytes of two individuals with the rare inab phenotype\"\nabbreviation_location = 68  \n\n# Use the predict_expansion function\nresult = predict_expansion(\n    text=example_text,\n    location=abbreviation_location,\n    tokenizer=tokenizer,\n    model=model,\n    device=device,\n    top_k=5  # Number of top predictions to return\n)\n\n# Display Results\nprint(\"=== Prediction Results ===\")\nprint(f\"Original Text: {result['original_text']}\")\nprint(f\"Masked Text: {result['masked_text']}\")\nprint(f\"Top Predictions: {result['predictions']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:32:16.975133Z","iopub.status.idle":"2025-09-08T17:32:16.975390Z","shell.execute_reply.started":"2025-09-08T17:32:16.975280Z","shell.execute_reply":"2025-09-08T17:32:16.975290Z"}},"outputs":[],"execution_count":null},{"id":"95aed684-9a02-4f30-aeba-697f0fc0b09f","cell_type":"code","source":"\n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d53678bb-cde3-4235-a836-e7606dff9502","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}